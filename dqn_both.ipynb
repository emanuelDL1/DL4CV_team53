{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple DQN for Both Environments\n",
    "==============================\n",
    "\n",
    "We will train a simple DQN for both environments of our project of openAI gym.\n",
    "First, let's import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-05 10:59:55,740] Making new env: Breakout-v0\n",
      "[2018-01-05 10:59:55,943] Making new env: SpaceInvaders-v0\n"
     ]
    }
   ],
   "source": [
    "#%% load required packages\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# create the environments\n",
    "env1 = gym.make('Breakout-v0').unwrapped\n",
    "env2 = gym.make('SpaceInvaders-v0').unwrapped\n",
    "actionsDim = 4 # number of actions for this environments\n",
    "# actually, SpaceInvaders has 6 actions, but the last 2 of them,\n",
    "# (left-shoot, right-shoot) aren't necessary to play the game\n",
    "imsize = 50 # parameter for the dqn image size (square images)\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "---------------------\n",
    "\n",
    "The first thing we will need to train our DQN is a so-called replay memory.\n",
    "This is a buffer that stores the states, actions and rewards our agent experiences\n",
    "as it interacts with the environment. By sampling data from this memory, we can\n",
    "create the batches we need to train our model. The sampling must be random, otherwise\n",
    "our data will be correlated, and the performance would decrease.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN model\n",
    "-------------\n",
    "\n",
    "The name DQN stands for \"Deep Q Network\". It is a network that tries to learn the so-called $Q$ function of our environment. The Q function, $Q^\\pi(s,a)$, gives us the expected total reward, when we are in state $s$, do action $a$, and then follow policy $\\pi$ afterwards.<br>\n",
    "From RL, it can be shown that the $Q^*$ function of the optimal (best) policy we are looking for, will satisfy the following equation:\n",
    "\n",
    "\\begin{align}Q^*(s,a) = r + \\gamma \\max_a Q^*(s',a)\\end{align}\n",
    "\n",
    "The difference between the two sides is called $\\delta$. The goal of our network is to minimize it, in order to determine the optimal Q function, with which, we can then determining the optimal policy, by finding the action that maximizes the Q function of a given state.<br>\n",
    "To train the network, we use the Huber loss, which has been shown to be more robust against noisy samples that the simple quadratic loss. The data we will use will come from the replay memory.<br>\n",
    "For the model, we use a CNN, so that we can feed the image directly as a state to the network. We use three conv layers with batchnorm, together with a final linear layer to compute the scores. The output is the Q function of the input state, for all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model we will be using\n",
    "        \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = (6,imsize,imsize), kernel_size = 5, stride = 2, pad = 0, num_classes = actionsDim):\n",
    "        # a simple dqn network. We use three conv layers with batch norm,\n",
    "        # and a final linear layer (we don't need softmax). The outputs\n",
    "        # will be the Q funtion for each possible action. We use 16 and\n",
    "        # 32 filters, with kernel size 5 and stride 2.\n",
    "        super(DQN, self).__init__()\n",
    "        # get input dims\n",
    "        C, H, W = input_dim\n",
    "        # layer 1: conv layer\n",
    "        self.conv1 = nn.Conv2d(C, 16, kernel_size=kernel_size, stride=stride, padding = (pad,pad))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        # update params\n",
    "        H = int((H + 2*pad - kernel_size) / stride + 1)\n",
    "        W = int((W + 2*pad - kernel_size) / stride + 1)\n",
    "        C = 16\n",
    "        # layer 2: conv\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=kernel_size, stride=stride, padding = (pad,pad))\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # update params\n",
    "        H = int((H + 2*pad - kernel_size) / stride + 1)\n",
    "        W = int((W + 2*pad - kernel_size) / stride + 1)\n",
    "        C = 32\n",
    "        # layer 3: conv\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=kernel_size, stride=stride, padding = (pad,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        # update params\n",
    "        H = int((H + 2*pad - kernel_size) / stride + 1)\n",
    "        W = int((W + 2*pad - kernel_size) / stride + 1)\n",
    "        C = 32\n",
    "        # layer 4: linear     \n",
    "        self.head = nn.Linear(H*W*C, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Input\n",
    "-----------------------------------------\n",
    "To train the network, we must be able to extract snapshots from the environment. We do that with the following function, which gets a snapshot from gym, transforms it to a square image of given size, and then to a pytorch tensor. Now, we have two environments but only one network! To dela with this situation, we stack the two images on a larger tensor with six channels. This will be the combined state that we will feed to the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input extraction\n",
    "\n",
    "# Define an image transformation\n",
    "myTransform = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize([imsize,imsize], interpolation=Image.CUBIC),\n",
    "                    #T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "    \n",
    "# a function to grab an image from the environment\n",
    "def get_screen():\n",
    "    # take a snapshot and transpose into torch order (CHW)    \n",
    "    screen1 = env1.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen2 = env2.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Convert to float, risize, grayscale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen1 = np.ascontiguousarray(screen1, dtype=np.float32) / 255\n",
    "    screen1 = torch.from_numpy(screen1)\n",
    "    screen2 = np.ascontiguousarray(screen2, dtype=np.float32) / 255\n",
    "    screen2 = torch.from_numpy(screen2)\n",
    "    # apply transformation (resize, etc.) on images\n",
    "    img1 = myTransform(screen1)\n",
    "    img2 = myTransform(screen2)\n",
    "    # stack the two images together\n",
    "    # this will be the image fed to the dqn\n",
    "    screen = torch.cat([img1, img2], 0)\n",
    "    # Add a batch dimension (BCHW), and return\n",
    "    return screen.unsqueeze(0).type(Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters, etc.\n",
    "-------------------------------------\n",
    "In this section, we define the hyperparameters for our model, and a policy function, that uses the model to find the next action, given the state. We use an e-greedy policy, with $\\epsilon$ decaying to zero exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded\n"
     ]
    }
   ],
   "source": [
    "# hyperparams and other utilities\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "# path to save model weiths\n",
    "PATH = 'myModel_Both'\n",
    "\n",
    "# build the model\n",
    "model = DQN()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# load pretrained weights, if they exist\n",
    "try:\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print('Pretrained weights loaded')\n",
    "except:\n",
    "    print('No pretrained weights avaiable, or weights dont match')\n",
    "\n",
    "# build the optimizer\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "# initialize the replay memory\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    # selects an action based on an e-greedy policy\n",
    "    # epsilon decays exponentionally towards zero, so that our\n",
    "    # policy becomes greedy as time passes\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(\n",
    "            Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(actionsDim)]])\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizer\n",
    "-----------------------------\n",
    "This function optimizes our model, by computing the Huber loss and then backporpagating. Notice that we need to compute two network outputs (for the current state and the next - expected state) to calculate the Huber loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "        \n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # get a batch from replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # extract states, actions and rewards in the batch\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t) for all actions,\n",
    "    # then we select the columns of actions taken\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute a mask of non-final states\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)))\n",
    "\n",
    "    # get the next states that are not final states (None)\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "\n",
    "    # Compute max Q(s',a) for all non-final next states (rest are zeros)\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "    # set volatile to false, so that we don't backprop through the next\n",
    "    # states (the DQN's inputs are the current states-actions)\n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        # gradient clipping\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train\n",
    "------------------------------\n",
    "We run episodes with the environment to train the model. Until each episode finishes, we store the transitions in the replay memory, and use them for training. We do a batch training step after each time-step, and not all together after the episode. This could improve performance.<br>\n",
    "Here, one episode finishes when the agent loses either one of the games. As reward, we consider the sum of the rewards assigned from both games. We normalize them in the interval $[-1, 1]$, so that they lie approximately in the same range. There may be better ways to assign the toal reward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode no. 0, duration: 260\n",
      "Episode no. 1, duration: 280\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe874ff2240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run some episodes and train the model!\n",
    "    \n",
    "num_episodes = 2 # in reality, this should be thousands!\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env1.reset()\n",
    "    env2.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    # the state is the difference image between current and prev screens\n",
    "    # this trick captures also the velocities of various objects\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward1, done1, _ = env1.step(action[0, 0])\n",
    "        # clip reward in range [-1, 1]\n",
    "        reward1 = min(-1, max(1, reward1/10))\n",
    "        reward1 = Tensor([reward1])\n",
    "        # similarly for env2\n",
    "        _, reward2, done2, _ = env2.step(action[0, 0])\n",
    "        # clip reward in range [-1, 1]\n",
    "        reward2 = min(-1, max(1, reward2/10))\n",
    "        reward2 = Tensor([reward2])\n",
    "        # get total reward and find if an env stopped\n",
    "        reward = reward1 + reward2 # there will be bether ways here\n",
    "        done = done1 or done2\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        # we optimize the network in place, not after the episodes\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            print('Episode no. ' + str(i_episode)+', duration: ' + str(t+1))\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plt.plot(list(range(len(episode_durations))), episode_durations)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('duration')\n",
    "plt.title('Episode duration during training')\n",
    "plt.show()\n",
    "# store weights\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation\n",
    "------------------------------\n",
    "Finally, we run one episode with the trained model live on the gym environment to see what it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press any key to close program...k\n",
      "The duration of this episode was: 292\n"
     ]
    }
   ],
   "source": [
    "#%% run trained model on a sample episode\n",
    "\n",
    "done = False\n",
    "t = 0 \n",
    "env1.reset()\n",
    "env2.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "while not done:\n",
    "    env1.render()\n",
    "    env2.render()\n",
    "    t += 1\n",
    "    last_screen = get_screen()\n",
    "    # Select and perform an action\n",
    "    action = select_action(state)\n",
    "    _, _, done1, _ = env1.step(action[0, 0])\n",
    "    _, _, done2, _ = env2.step(action[0, 0])\n",
    "    done = done1 or done2\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "# we're done\n",
    "input('Press any key to close program...')\n",
    "env1.render(close=True)\n",
    "env1.close()\n",
    "env2.render(close=True)\n",
    "env2.close()\n",
    "print('The duration of this episode was: ' + str(t))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
